{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianyaoZhang/DeepRL/blob/master/week5_policy_based/practice_reinforce_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5bIcc2znsy0"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJpbzMibnsy2",
        "outputId": "84715983-7517-4429-dcc6-8ae1e1ea8a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BImiL2jZnsy4"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwvn8wSKnsy4"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ylnfADPHnsy4",
        "outputId": "40c2840e-477a-49a8-f460-d8b4644a428b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2d80be5f50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17qgW7U4nsy5"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-xhukcFnsy5"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I7picK9Tnsy6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6I8F0OMgnsy6"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(4,64),\n",
        "  nn.Linear(64,32),\n",
        "  nn.Linear(32,2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s91fh7Funsy7"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjdEQTZAnsy7"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Hd5Y_Suqnsy7"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "        input = torch.tensor(states,dtype=torch.float32)\n",
        "        logits = model(input)\n",
        "        probs = nn.functional.softmax(logits,1)\n",
        "    return probs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-cseKz9Gnsy8"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTYoZsX-nsy8"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bzkvwYOjnsy8"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = torch.distributions.categorical.Categorical(torch.tensor(action_probs)).sample().item()\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zIE_yz3unsy8"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU1YGHVHnsy9"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9fhJyrPFnsy9"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cumulative_rewards = rewards.copy()\n",
        "    for i in range(len(cumulative_rewards)-1,0,-1):\n",
        "        cumulative_rewards[i-1] += cumulative_rewards[i] * gamma\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hc2QvW3mnsy-",
        "outputId": "3f3f55ae-4835-4d51-f7d8-88ec55137333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPBiY1Znsy-"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yk3ckiCensy-"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4lxucIpnnsy-"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "loss_display = 0\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = - entropy_coef * torch.mean(log_probs * probs)\n",
        "    loss = - torch.mean(log_probs_for_actions * cumulative_returns) + entropy\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1LsmLensy_"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1rWOqi8Tnsy_",
        "outputId": "904f00ab-5681-48b1-fe14-02ef928e6d10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:23.320\n",
            "mean reward:67.230\n",
            "mean reward:63.740\n",
            "mean reward:157.880\n",
            "mean reward:108.360\n",
            "mean reward:226.690\n",
            "mean reward:236.930\n",
            "mean reward:631.370\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UEzJ_9ansy_"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4N6Cj3i9nsy_"
      },
      "outputs": [],
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ssy0-OzZnsy_",
        "outputId": "27c79190-56e8-4878-b69e-602b5e8af469",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.70.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.70.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gWsHZO1gnsy_",
        "outputId": "31c6c69b-6549-4a85-cb4b-811db4b2b407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your average reward is 958.95 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ]
        }
      ],
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'tianyao@buaa.edu.cn', 'tFjLtF10xmPs7kCi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWtPmCF5nsy_"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}